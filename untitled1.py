# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fNcMJlKL92LVf3oc45ByZqHl2Zh1khg7
"""

# 1. Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Set style for better visualizations
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12, 6)

# 2. Load the dataset
df = pd.read_csv('weatherAUS.csv')

# 3. Data Preprocessing & Feature Selection

# Drop columns with too many missing values
df.drop(['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'], axis=1, inplace=True)

# Drop rows with missing target
df.dropna(subset=['RainTomorrow'], inplace=True)

# Fill numerical missing values with mean
for col in df.select_dtypes(include=np.number).columns:
    df[col].fillna(df[col].mean(), inplace=True)

# Fill categorical missing values with mode
for col in df.select_dtypes(include='object').columns:
    if col != 'RainTomorrow':  # Skip target variable
        df[col].fillna(df[col].mode()[0], inplace=True)

# Remove irrelevant features
df.drop(['Date', 'Location', 'RISK_MM'], axis=1, inplace=True)

# Transform categorical variables into numerical
le = LabelEncoder()
df['RainToday'] = le.fit_transform(df['RainToday'])
df['RainTomorrow'] = le.fit_transform(df['RainTomorrow'])
df['WindGustDir'] = le.fit_transform(df['WindGustDir'])
df['WindDir9am'] = le.fit_transform(df['WindDir9am'])
df['WindDir3pm'] = le.fit_transform(df['WindDir3pm'])

# 4. Data Visualization

# Bar plot for RainTomorrow distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='RainTomorrow', data=df)
plt.title('Distribution of Rain Tomorrow (0 = No, 1 = Yes)')
plt.xlabel('Rain Tomorrow')
plt.ylabel('Count')
plt.show()

# Bar plot for RainToday vs RainTomorrow
plt.figure(figsize=(8, 5))
sns.countplot(x='RainToday', hue='RainTomorrow', data=df)
plt.title('Rain Today vs Rain Tomorrow')
plt.xlabel('Rain Today (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.legend(title='Rain Tomorrow', labels=['No', 'Yes'])
plt.show()

# Heatmap with better readability
plt.figure(figsize=(16, 12))
corr = df.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, annot=True, fmt=".2f", cmap='coolwarm',
            vmin=-1, vmax=1, center=0, linewidths=.5)
plt.title('Feature Correlation Heatmap')
plt.tight_layout()
plt.show()

# Histogram of numerical features
numerical_features = df.select_dtypes(include=np.number).columns
df[numerical_features].hist(bins=20, figsize=(15, 10))
plt.suptitle('Distribution of Numerical Features', y=1.02)
plt.tight_layout()
plt.show()

# 5. Splitting the dataset
X = df.drop('RainTomorrow', axis=1)
y = df['RainTomorrow']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. Model Training
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 7. Model Evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Plot feature importance
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.nlargest(10).plot(kind='barh')
plt.title('Top 10 Feature Importance')
plt.show()

# 8. Build the Predictive Model
def predict_rain(input_data):
    input_df = pd.DataFrame([input_data], columns=X.columns)
    prediction = model.predict(input_df)[0]
    return "Rain" if prediction == 1 else "No Rain"

# Example prediction
example = X_test.iloc[0]
print("\nExample features:\n", example)
prediction = predict_rain(example)
print("\nPrediction for sample:", prediction)